model_id: "openai/gpt-oss-20b"                           # Hugging Face model id
mlflow_uri: ""                       # MLflow tracking server URI
mlflow_experiment_name: "" # MLflow experiment name
# sagemaker specific parameters
output_dir: "/opt/ml/model"                       # path to where SageMaker will upload the model 
checkpoint_dir: "/opt/ml/checkpoints/"            # directory for saving training checkpoints
train_dataset_path: "/opt/ml/input/data/train/"   # path to where S3 saves train dataset
val_dataset_path: "/opt/ml/input/data/val/"       # path to where S3 saves test dataset
token: "hf_*****"                              # Hugging Face API token
merge_weights: true                               # merge weights in the base model
# training parameters
apply_truncation: true                           # apply truncation to datasets
use_snapshot_download: false                     # Use snapshot_download to download the model
attn_implementation: "eager"    # attention implementation type
learning_rate: 1e-4                              # learning rate scheduler
num_train_epochs: 10                             # number of training epochs
per_device_train_batch_size: 4                   # batch size per device during training
per_device_eval_batch_size: 2                    # batch size for evaluation
gradient_accumulation_steps: 4                   # number of steps before performing a backward/update pass
gradient_checkpointing: true                     # use gradient checkpointing
torch_dtype: "bfloat16"                          # float precision type
bf16: true                                       # use bfloat16 precision
tf32: true                                       # use tf32 precision
ignore_data_skip: true                           # skip data loading errors
logging_strategy: "steps"                        # logging strategy
logging_steps: 1                                 # log every N steps
log_on_each_node: false                          # disable logging on each node
ddp_find_unused_parameters: false                # DDP unused parameter detection
save_total_limit: 1                              # maximum number of checkpoints to keep
save_steps: 100                                  # Save checkpoint every this many steps
warmup_steps: 50                                 # number of warmup steps
weight_decay: 0.01                               # weight decay coefficient
# LoRA parameters
load_in_4bit: true                               # enable 4-bit quantization
use_mxfp4: true                                  # use MXFP4 quantization
lora_r: 8                                        # LoRA rank
lora_alpha: 16                                   # LoRA alpha parameter
lora_dropout: 0.05                               # LoRA dropout rate
